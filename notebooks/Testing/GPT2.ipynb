{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571b144e-f2b8-4039-8d19-5ea6fd1f15d3",
   "metadata": {},
   "source": [
    "# GPT-2\n",
    "<b>Date:</b> October 6, 2023\\\n",
    "<b>Author:</b> Dimitris Lymperopoulos\\\n",
    "<b>Description:</b> A notebook for experimentation with GPT-2 huggingface transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629888f-d9b5-4cfa-a0f9-b51bbec30dec",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "Run the cell below to download and install the necessary python packages for GPT-2 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8979c-e2ef-4644-a22a-dffd18dda9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install pylev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab990c8-737f-4607-b501-877094bb2714",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7adadec8-a4d4-43b4-bed1-edd4214ead0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from pylev import levenshtein as lev_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0ca81c-9610-4b1c-84ab-a4ec6fcc4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimli\\anaconda3\\envs\\nlp_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%run ../functions/GPT2_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc355a06-e611-463d-a5dd-078297400e24",
   "metadata": {},
   "source": [
    "## Additional Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bab886f0-6553-40d3-a0e3-974787688079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_string='gpt2', cuda=False):\n",
    "    \"\"\"\n",
    "    A function that initializes a LM and a Tokenizer based on GPT2. \n",
    "\n",
    "    :param model_string: string representing the base model for the transformer and the tokenizer\n",
    "    :param cuda: boolean value, determining whether or not to use gpu for model inference\n",
    "    :return: the pretrained model and tokenizer\n",
    "    \"\"\"\n",
    "    if model_string.startswith(\"gpt2\"):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_string)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_string)\n",
    "    else:\n",
    "        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_string)\n",
    "        model = OpenAIGPTLMHeadModel.from_pretrained(model_string)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.to('cuda')\n",
    "    print(\"Model init\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93bc136b-e7f7-4999-8f62-e4f9fb41a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_scoring(model, tokenizer, text, cuda=False):\n",
    "    \"\"\"\n",
    "    A function that uses the given LM and Tokenizer to compute the probability of a given sentence.\n",
    "\n",
    "    :param model: a pretrained transformer model\n",
    "    :param tokenizer: a pretrained tokenizer\n",
    "    :param text: a string representing the sentence whose probability will be computed\n",
    "    :param cuda: boolean value, determining whether or not to use gpu for model inference\n",
    "    :return: the computed loss of the sentence and log_probability of the last token\n",
    "    \"\"\"\n",
    "    assert model is not None\n",
    "    assert tokenizer is not None\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    if cuda:\n",
    "        tokens = tokens.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, labels=tokens)\n",
    "    loss, logits = outputs[:2]\n",
    "    loss, log_prob = loss.item(), logits[0, -1, tokens[0, -1]].item()\n",
    "    return loss, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc7523-a825-4bc2-a0c2-a6d688439288",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d5ac28e-8266-4298-b28c-37a21c77b8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model init\n"
     ]
    }
   ],
   "source": [
    "# initialize model and tokenizer\n",
    "model, tokenizer = model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95958223-cb07-47c6-886f-229feff41141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Sentence: A dog is embraced by the woman\n",
      "Loss: 5.209733486175537\n",
      "\n",
      "2.\n",
      "Sentence: A woman is embraced by the table\n",
      "Loss: 5.908300876617432\n",
      "\n",
      "3.\n",
      "Sentence: A is embraced the woman\n",
      "Loss: 7.431522369384766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "sentences = ['A dog is embraced by the woman', 'A woman is embraced by the table', 'A is embraced the woman']\n",
    "\n",
    "for i, s in enumerate(sentences):\n",
    "    loss, prob = sent_scoring(model, tokenizer, s)\n",
    "    print(\"{}.\\nSentence: {}\\nLoss: {}\".format(i+1, s, loss, prob), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95b0db67-295f-4593-8dfc-a1a9898c727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Sentence: A table is hugging the man\n",
      "Loss: 6.892320156097412\n",
      "\n",
      "2.\n",
      "Sentence: A woman is hugging the man\n",
      "Loss: 5.655217170715332\n",
      "\n",
      "3.\n",
      "Sentence: The man is hugged by the woman\n",
      "Loss: 4.746158123016357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "sentences = ['A table is hugging the man', 'A woman is hugging the man', 'The man is hugged by the woman']\n",
    "\n",
    "for i, s in enumerate(sentences):\n",
    "    loss, prob = sent_scoring(model, tokenizer, s)\n",
    "    print(\"{}.\\nSentence: {}\\nLoss: {}\".format(i+1, s, loss, prob), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "051a0e21-7f15-43ec-8ee7-b8eaf66fd055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance: 5\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3 - Levenshtein distance as closeness metric\n",
    "sentence_pair = ['A woman is hugging the man', 'The man is hugged by the woman']\n",
    "s1 = sentence_pair[0].lower().split()\n",
    "s2 = sentence_pair[1].lower().split()\n",
    "\n",
    "# TODO: perhaps lemmatize before computing the distance (?)\n",
    "d = lev_dist(s1, s2)\n",
    "print(\"Levenshtein Distance: {}\".format(d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
