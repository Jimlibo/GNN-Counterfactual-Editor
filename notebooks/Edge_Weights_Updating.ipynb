{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0659989-d61d-41fe-a003-ccd9fc1d32ed",
   "metadata": {},
   "source": [
    "# Analysis and Creation of updating formula for graph edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dc1c0-7436-4858-90fb-ac46e2a99c25",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf4f91-ecbb-4c70-beaf-333adf4c1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install networkx\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ed90d-2860-44e9-a6a9-7b250119a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polyjuice_nlp\n",
    "!pip install torch\n",
    "!pip install evaluate\n",
    "!pip install bert_score\n",
    "!python -m spacy download en_core_web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207c67a-41c8-4b59-8e99-13af2cc75c7e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a592d877-5b2e-4a19-abcf-c040232dfd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Metric-related imports\n",
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from evaluate import load\n",
    "from pylev import levenshtein as lev_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c20e5d0b-61b7-47ae-8f99-1e98030f0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run GPT2_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0bdef-b3f2-43d1-aa10-07dbe54b6c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run graph_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "08b8eadd-dc09-40b7-982c-1e15e4a293fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961da7b-c435-40bb-85c2-ff7c137a7b7a",
   "metadata": {},
   "source": [
    "## Edge Updating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b310acda-a6d3-48b3-842a-189bc0b58946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_edges(edges, substitutions, lr, baseline_metric_value, current_metric_value):\n",
    "    \"\"\"\n",
    "    A function that takes as input a list of weighted edges along with other parameters, and uses\n",
    "    these parameters to update the edge weights.\n",
    "\n",
    "    :param edges: an iterable containing weighted edges as tuples\n",
    "    :param substitutions: a dictionary with edges as keys, and their substitution occurence as values\n",
    "    :param lr: float value, representing the learning rate for the weight updating\n",
    "    :param baseline_metric_value: a float, representing the baseline evaluation metric value\n",
    "    :param current_metric_value: a float, representing the current evaluation metric value\n",
    "    :returns: a list of tuples, where each tuple represents an updated weighted edge\n",
    "    \"\"\"\n",
    "    \n",
    "    updated_edges = list()\n",
    "    for (u, v, w) in edges:\n",
    "        try:\n",
    "            # get substitution occurences for each edge\n",
    "            edge_subs = substitutions[(u,v)] \n",
    "            # updating formula\n",
    "            new_w = w - lr * (baseline_metric_value - current_metric_value) / edge_subs   # - for minimizing, + for maximizing\n",
    "            # add the updated edge to the list\n",
    "            updated_edges.append((u, v, new_w))\n",
    "        except KeyError:\n",
    "            print(\"Something went wrong during updating of edges' weights\")\n",
    "    \n",
    "    return updated_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd687acd-4f4f-4bff-acab-ce0b5b9db89a",
   "metadata": {},
   "source": [
    "## Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8e54ccdc-b68b-481f-b972-cd9d5b73ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_agnostic_metrics(data, counter_data):\n",
    "    \"\"\"\n",
    "    A function that takes as input the original and the counter data and returns a dictionary with \n",
    "    model-agnostic metrics such as closeness and fluency.\n",
    "\n",
    "    :param data: dataframe containing one column with the original data\n",
    "    :param counter_data: dataframe containing one column with the counter data\n",
    "    :returns: dictionary containing model-agnostic metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract sentences and counter-sentences from the data and check that they are of the same length\n",
    "    sentences = [elem[0] for elem in data.values.tolist()]\n",
    "    counter_sentences= [elem[0] for elem in counter_data.values.tolist()]\n",
    "\n",
    "    assert len(sentences) == len(counter_sentences)\n",
    "\n",
    "    sent_length = len(sentences)\n",
    "\n",
    "    # compute average levenshtein distance as a measurement of closeness\n",
    "    avg_lev = sum(list(map(lambda x: lev_dist(x[0], x[1]), zip(sentences, counter_sentences)))) / sent_length\n",
    "\n",
    "    # compute average fluency\n",
    "    model, tokenizer = model_init()\n",
    "    #avg_fluency = sum(list(map(lambda x: sent_scoring(model, tokenizer, x)[0], counter_sentences))) / sent_length\n",
    "    avg_fluency = sum(\n",
    "        list(map(lambda x: abs(sent_scoring(model, tokenizer, x[0])[0] - sent_scoring(model, tokenizer, x[1])[0]), zip(sentences, counter_sentences)))\n",
    "    ) / len(sentences)\n",
    "\n",
    "    # compute average bertscore\n",
    "    avg_bertscore = 1 - sum(bertscore.compute(predictions=counter_sentences, references=sentences, model_type=\"distilbert-base-uncased\")['f1']) / sent_length\n",
    "    \n",
    "    # create metrics dictionary\n",
    "    metrics = {\n",
    "        'levenshtein': avg_lev,     # we want this to be as low as possible\n",
    "        'fluency': avg_fluency,     # we want this to be as low as possible (it is the difference |original_fluency - counter_fluency|)\n",
    "        'bertscore': avg_bertscore  #  we want this to be as low as possible\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "702e09a8-361a-4d2b-8e4b-11781025b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_related_metrics(original_p, counter_p):\n",
    "    \"\"\"\n",
    "    A function that takes as input the original predictions and the new ones, and returns a dictionary with \n",
    "    model-related metrics such as flip-rate.\n",
    "\n",
    "    :param original_p: list containing the predictions for the original data\n",
    "    :param counter_p: dataframe containing the predictions for the counter data\n",
    "    :returns: dictionary containing model-related metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # check that predictions and counter_predictions are of the same length\n",
    "    assert len(original_p) == len(counter_p)\n",
    "    \n",
    "    # compute flip_rate\n",
    "    flip_rate_percent = sum(x[0] != x[1] for x in zip(original_p, counter_p)) / len(original_p)\n",
    "\n",
    "    # create metrics dictionary\n",
    "    metrics = {\n",
    "        'flip-rate': flip_rate_percent\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a5c6b7e0-426c-470e-a7ed-9b1d04853969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counterfactual_metric(metrics):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dictionary containing evalutation metrics, and returns\n",
    "    a combination of those metrics.\n",
    "\n",
    "    :param metrics: dictionary containing different evaluation metrics such as fluency, flip-rate, etc.\n",
    "    :returns: float value, computed as a combination of the metrics in the given dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 / (1/metrics['bertscore'] + 1/metrics['fluency'])\n",
    "    #return len(metrics) / sum(1/v for v in metrics.values())   # compute final metric as the harmonic mean of the given metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "05116c5e-6823-452c-9bf4-c94612059d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_metric(data, model_required=False, preprocessor=None, model=None, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dataframe with the textual data, and computes a metric based on a bipartite graph,\n",
    "    where the edge weights represent the distance between words (nodes) as extracted from wordnet.\n",
    "\n",
    "    :param data: pd.DataFrame() containing one column with the textual data\n",
    "    :param model_required: boolean value specifing whether a pretrained model is also required for the metric computation\n",
    "    :param preprocessor: a custom class that implements the necessary preprocessing of the data\n",
    "    :param model: a pretrained model on the dataset \n",
    "    :returns: a float value representing the computed evaluation metric\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = [elem[0] for elem in data.values.tolist()]\n",
    "    counter_sents, _, _, _= get_edits(sents, pos='noun', thresh=1, antonyms=antonyms)\n",
    "    \n",
    "    counter_data_df = pd.DataFrame({\n",
    "        'counter_sents': counter_sents\n",
    "    })\n",
    "    metrics = generate_model_agnostic_metrics(data, counter_data_df)\n",
    "\n",
    "    if model_required:\n",
    "        # first process the original data and get model predictions\n",
    "        processed_data = preprocessor.process(data)\n",
    "        original_preds = model.predict(processed_data)\n",
    "    \n",
    "        # do the same but for the counterfactual-generated data\n",
    "        processed_counter_data = preprocessor.process(counter_data_df)\n",
    "        counter_preds = model.predict(processed_counter_data)\n",
    "\n",
    "        # add model-related metrics to the current_metrics dictionary\n",
    "        metrics.update(generate_model_related_metrics(original_preds, counter_preds))\n",
    "            \n",
    "    return get_counterfactual_metric(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5b9a9-cb3d-4343-add6-a26ac06be324",
   "metadata": {},
   "source": [
    "## Graph-Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "38744b17-f2fd-4244-b94d-7bcb333444e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(data, pos, antonyms=False):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dataframe and a part-of-speech tag, and creates a bipartite graph\n",
    "    with the possible substitution words and their candidates.\n",
    "\n",
    "    :param data: pd.DataFrame() containing one column with the textual data\n",
    "    :param pos: string that specifies which part-of-speech shall be considered for substitution (noun, verb, adv)\n",
    "    :param antonyms: boolean value specifing whether or not to use antonyms in the candidate substitutions \n",
    "    :returns: a dictionary containing the graph, along with other related features\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = [elem[0] for elem in data.values.tolist()]\n",
    "    lst = None\n",
    "    \n",
    "    # use appropriate function based on pos to get the list of the specified pos words from the data\n",
    "    if pos == 'adv':\n",
    "        lst = create_attributes_list(sentences)\n",
    "    elif pos == 'verb':\n",
    "        lst = create_verb_list(sentences)\n",
    "    elif pos == 'noun':\n",
    "        lst = create_singular_list(sentences) \n",
    "    else:\n",
    "        raise AttributeError(\"pos '{}' is not supported!\".format(pos)) \n",
    "    \n",
    "    weights = []\n",
    "    syn0 = list(lst)\n",
    "    syn1 = list(get_antonym_list(lst)) if antonyms else list(lst)\n",
    "        \n",
    "    all_syn0, d0, ind0 = get_synsets(syn0, return_index=True)\n",
    "    all_syn1, d1, ind1= get_synsets(syn1, return_index=True)\n",
    "    names0 = ['G0_'+str(i) for i in range(len(all_syn0))]  # give unique names for each synset of the two sets\n",
    "    names1 = ['G1_'+str(i) for i in range(len(all_syn1))]\n",
    "\n",
    "    word_to_node0 = dict()\n",
    "    word_to_node1 = dict()\n",
    "    for t in zip(names0, ind0):\n",
    "        word_to_node0[syn0[t[1]]] = t[0]\n",
    "\n",
    "    for t in zip(names1, ind1):\n",
    "        word_to_node1[syn1[t[1]]] = t[0]\n",
    "        \n",
    "    \n",
    "    # synset as key, word as val\n",
    "    combinations_nodes = all_combinations(names0, names1)        # all combinations of names\n",
    "    combinations_synsets = all_combinations(all_syn0, all_syn1)  # all combinations of synsets\n",
    "    weights = [1] * len(combinations_nodes)\n",
    "   \n",
    "    G, min_list_nodes = bipartite_graph(names0, names1, combinations_nodes, weights) # create bipartite graph\n",
    "\n",
    "    graph_dict = {\n",
    "        'graph': G,\n",
    "        'min_list_nodes': min_list_nodes,\n",
    "        'weights': weights,\n",
    "        'd0': d0,\n",
    "        'd1': d1,\n",
    "        'comb_nodes': combinations_nodes,\n",
    "        'comb_syn': combinations_synsets,\n",
    "        'word_to_node0': word_to_node0,\n",
    "        'word_to_node1': word_to_node1\n",
    "    }\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0585d7ff-4bc7-40e9-80c3-c15b3a282cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_matching(graph_dict):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dictionary containing a graph and other related features, and uses\n",
    "    a minimum graph matching algorithm to return candidate substitutions, along with other graph features.\n",
    "\n",
    "    :param graph_dict: a dictionary containing a bipartite graph and other related features\n",
    "    :returns: a list of feasible substitutions, mappings of synsets to their words, and a tuple containing the graph, a min_list_nodes and the minimum matching\n",
    "    \"\"\"\n",
    "    \n",
    "    # unpack dictionary items\n",
    "    G = graph_dict['graph']\n",
    "    min_list_nodes = graph_dict['min_list_nodes']\n",
    "    weights = graph_dict['weights']\n",
    "    d0 = graph_dict['d0']\n",
    "    d1 = graph_dict['d1']\n",
    "    combinations_nodes = graph_dict['comb_nodes']\n",
    "    combinations_synsets = graph_dict['comb_syn']\n",
    "\n",
    "    # find min weight match\n",
    "    min_match = minimum_match(G, min_list_nodes)                                     \n",
    "    match_tuple = dict_to_tuple(min_match)\n",
    "    \n",
    "    new_match=[]\n",
    "    for i in match_tuple:\n",
    "        new_match.append(tuple(sorted(i)))\n",
    "        new_match = remove_duplicates(new_match)\n",
    "\n",
    "    positions = pos_in_list(combinations_nodes, list(new_match))\n",
    "    substitution_synsets = []\n",
    "    \n",
    "    for i in positions:\n",
    "        substitution_synsets.append((weights[i], combinations_synsets[i][0], combinations_synsets[i][1]))     \n",
    "    # sum_similarities, avg_similarity, best_matched_synsets = total_graph_weight(positions, weights, combinations_synsets)\n",
    "    \n",
    "    return substitution_synsets, d0, d1, (G, min_list_nodes, new_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1558be4-13c0-4b7d-b9df-1d738af179af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counterfactuals(graph_dict, data, pos):\n",
    "    \"\"\"\n",
    "    A function that takes as input a dictionary containing graph information, along with a dataframe and a part-of-speech tag,\n",
    "    and uses them to generate counterfactual edits from the data.\n",
    "\n",
    "    :param graph_dict: a dictionary containing a bipartite graph and other related features\n",
    "    :param data: pd.DataFrame() containing one column with the textual data\n",
    "    :param pos: string that specifies which part-of-speech shall be considered for substitution (noun, verb, adv)\n",
    "    :returns: a dataframe with the generated counterfactual data, a list of selected edges from the graph and a dictionary containing substitution occurrence\n",
    "    \"\"\"\n",
    "    \n",
    "    G = graph_dict['graph']\n",
    "    w2n0 = graph_dict['word_to_node0']\n",
    "    w2n1 = graph_dict['word_to_node1']\n",
    "    sentences = [elem[0] for elem in data.values.tolist()] \n",
    "    \n",
    "    # find best matching and generate edits\n",
    "    substitution_synsets, d0, d1, g = generate_graph_matching(graph_dict)\n",
    "    all_swaps, if_change, attr_counter, substitutions = external_swaps(sentences, pos, substitution_synsets, d0, d1, thresh=4)\n",
    "    \n",
    "\n",
    "    counter_data = pd.DataFrame({\n",
    "        'counter_sents': all_swaps\n",
    "    })\n",
    "    \n",
    "    subs_as_nodes = {(w2n0[k[0]], w2n1[k[1]]): v for (k,v) in substitutions.items()}\n",
    "\n",
    "    selected_edges = []\n",
    "    for (u,v) in subs_as_nodes.keys():\n",
    "        w = G.get_edge_data(u, v, default=0)['weight']\n",
    "        selected_edges.append((u, v, w))\n",
    "\n",
    "    return counter_data, selected_edges, subs_as_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "723864c4-0a26-4c30-875c-af9cf90130e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(graph_dict, data, pos, preprocessor=None, model=None, learning_rate=0.1, th=0.005, max_iterations=100, model_required=False):\n",
    "    \"\"\"\n",
    "    A function that represents the training process for the graph edges. It gets predictions for the original data\n",
    "    then uses a graph approach to generate counter data and get predictions for them. To get the current_metric\n",
    "    it compares the two predictions and based on those updates the weights of the selected edges.\n",
    "    \n",
    "    :param graph_dict: a dictionary containing the bipartite graph along with other variables and characteristics\n",
    "    :param data: a dataframe containing the textual examples we will use to train the graph\n",
    "    :param pos: a string specifing which part-of-speech shall be considered for substitutions (noun, verb, adv)\n",
    "    :param preprocessor: a custom class that implements the necessary preprocessing of the data\n",
    "    :param model: a pretrained model on the dataset\n",
    "    :param learning_rate: float value defining how fast or slow the edge weights will be updated\n",
    "    :param th: float value defining a threshold, where if the difference |baseline - current| get smaller, the training stops\n",
    "    :param max_iterations: integer value representing the maximum number of iterations for the training procedure\n",
    "    :param model_required: boolean value for whether or not to compute model-related metrics\n",
    "    :returns: the graph_dictionary with the fine-tuned (post-training) graph along with the rest of its features\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate counter data and compute model_agnostic metrics\n",
    "    baseline_metric = get_baseline_metric(data, model_required=model_required, preprocessor=preprocessor, model=model)\n",
    "    current_metric = baseline_metric + 2 * th   # initialize current_metric so that the dif |baseline-current| is bigger than th\n",
    "    \n",
    "    iterations = 0\n",
    "    next_baseline_metric = baseline_metric\n",
    "    while abs(current_metric - baseline_metric) >= th and iterations < max_iterations:\n",
    "        print(\"ITERATION {}\".format(iterations))\n",
    "\n",
    "        updated_edges = []\n",
    "        baseline_metric = next_baseline_metric\n",
    "\n",
    "        while nx.is_bipartite(graph_dict['graph']):\n",
    "            try:\n",
    "                # produce new counter_data and compute current_metric valule\n",
    "                counter_data, selected_edges, substitutions = generate_counterfactuals(graph_dict, data, pos)\n",
    "                current_metrics_dict = generate_model_agnostic_metrics(data, counter_data)\n",
    "        \n",
    "                # if needed, compute model-related metrics as well\n",
    "                if model_required:\n",
    "                    processed_counter_data = preprocessor.process(counter_data)\n",
    "                    counter_preds = model.predict(processed_counter_data)\n",
    "            \n",
    "                    # add model-related metrics to the current_metrics dictionary\n",
    "                    current_metrics_dict.update(generate_model_related_metrics(original_preds, counter_preds))  \n",
    "        \n",
    "                # compute the final metric as a combination of the previously computed metrics\n",
    "                current_metric = get_counterfactual_metric(current_metrics_dict)\n",
    "    \n",
    "                g = graph_dict['graph']\n",
    "                g.remove_edges_from(selected_edges)\n",
    "                new_edges = update_edges(selected_edges, substitutions, learning_rate, baseline_metric, current_metric)\n",
    "                \n",
    "                graph_dict['graph'] = g\n",
    "                updated_edges.extend(new_edges)\n",
    "            except:\n",
    "                graph_dict['graph'] = g\n",
    "                break\n",
    "            \n",
    "        g = graph_dict['graph']\n",
    "        # print(updated_edges)\n",
    "        g.add_weighted_edges_from(updated_edges)\n",
    "        graph_dict['graph'] = g\n",
    "\n",
    "        # update baseline_metric value and iterations\n",
    "        next_baseline_metric = min(baseline_metric, current_metric)\n",
    "        iterations += 1\n",
    "\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa73f1-ab9f-416c-9d71-5580febe9f3a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "db9e3960-f3b9-41c8-9ac2-e6c88a209a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 0\n"
     ]
    }
   ],
   "source": [
    "POS = 'adv'\n",
    "MAX_ITER = 3\n",
    "ANTONYMS = True\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'sents': [\n",
    "        'A great man was standing in a tall and magnificent hill, gazing upon the sad and destructive army',\n",
    "        'The clever boy was wondering when the fat dog would return with the big stick',\n",
    "        'A small town was standing next to the large river and the tall building'\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "gd = create_graph(data=df, pos=POS, antonyms=ANTONYMS)\n",
    "trained_gd = train_graph(graph_dict=gd, data=df, pos=POS, max_iterations=MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "06f1f84e-2a53-42e0-92db-ec72cf720556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:\n",
      "A great man was standing in a tall and magnificent hill, gazing upon the sad and destructive army\n",
      "COUNTER:\n",
      "a large man was standing in a glad and magnificent hill, gazing upon the sad and destructive army.\n",
      "===============================================================================================================\n",
      "ORIGINAL:\n",
      "The clever boy was wondering when the fat dog would return with the big stick\n",
      "COUNTER:\n",
      "the clever boy was wondering when the short dog would return with the big stick.\n",
      "===============================================================================================================\n",
      "ORIGINAL:\n",
      "A small town was standing next to the large river and the tall building\n",
      "COUNTER:\n",
      "a little town was standing next to the small river and the glad building.\n",
      "===============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "counter_data, selected_edges, subs = generate_counterfactuals(trained_gd, df, POS)\n",
    "for i in range(df.shape[0]):\n",
    "    print(\"ORIGINAL:\")\n",
    "    print(df['sents'][i])\n",
    "    print(\"COUNTER:\")\n",
    "    print(counter_data['counter_sents'][i])\n",
    "    print(\"===============================================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "291ba437-c82a-4b60-a200-a61e21b4efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline metric value: 0.07030562703168462\n",
      "Fine-tuned metric value: 0.06654568031053844\n",
      "Difference: 0.0037599467211461846\n"
     ]
    }
   ],
   "source": [
    "final_metric = get_counterfactual_metric(generate_model_agnostic_metrics(df, counter_data))\n",
    "baseline_metric = get_baseline_metric(df)\n",
    "\n",
    "print(\"Baseline metric value: {}\".format(baseline_metric))\n",
    "print(\"Fine-tuned metric value: {}\".format(final_metric))\n",
    "print(\"Difference: {}\".format(abs(baseline_metric - final_metric)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
