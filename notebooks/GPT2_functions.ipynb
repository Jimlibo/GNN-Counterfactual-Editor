{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea47843-0157-4366-aece-75245f0ecd8b",
   "metadata": {},
   "source": [
    "# GPT2 Functions\n",
    "<b>Date:</b> October 6, 2023\\\n",
    "<b>Author:</b> Dimitris Lymperopoulos\\\n",
    "<b>Description:</b> A notebook containing gpt2-related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc552f-0f7b-4914-8d1b-bb76adbf9578",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac428e93-1849-46e0-9459-82cfaa19c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimli\\anaconda3\\envs\\nlp_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from pylev import levenshtein as lev_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e26e3-fc20-4601-bc31-bd129edc8e75",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e742b4dd-1159-4d3b-beac-f6384accef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_string='gpt2', cuda=False):\n",
    "    \"\"\"\n",
    "    A function that initializes a LM and a Tokenizer based on GPT2. \n",
    "\n",
    "    :param model_string: string representing the base model for the transformer and the tokenizer\n",
    "    :param cuda: boolean value, determining whether or not to use gpu for model inference\n",
    "    :return: the pretrained model and tokenizer\n",
    "    \"\"\"\n",
    "    if model_string.startswith(\"gpt2\"):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_string)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_string)\n",
    "    else:\n",
    "        tokenizer = OpenAIGPTTokenizer.from_pretrained(model_string)\n",
    "        model = OpenAIGPTLMHeadModel.from_pretrained(model_string)\n",
    "    model.eval()\n",
    "    if cuda:\n",
    "        model.to('cuda')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ba3d0d-c18b-44ac-9911-29709d1149c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_scoring(model, tokenizer, text, cuda=False):\n",
    "    \"\"\"\n",
    "    A function that uses the given LM and Tokenizer to compute the probability of a given sentence.\n",
    "\n",
    "    :param model: a pretrained transformer model\n",
    "    :param tokenizer: a pretrained tokenizer\n",
    "    :param text: a string representing the sentence whose probability will be computed\n",
    "    :param cuda: boolean value, determining whether or not to use gpu for model inference\n",
    "    :return: the computed loss of the sentence and log_probability of the last token\n",
    "    \"\"\"\n",
    "    assert model is not None\n",
    "    assert tokenizer is not None\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    if cuda:\n",
    "        tokens = tokens.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, labels=tokens)\n",
    "    loss, logits = outputs[:2]\n",
    "    loss, log_prob = loss.item(), logits[0, -1, tokens[0, -1]].item()\n",
    "    return loss, log_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
